
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage[]{setspace} %provides commands to set line spacing
\usepackage[left=0.75in, right=0.75in]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumitem}





\begin{document}

\begin{center}
    \LARGE{Homework 5}\\[1em]
    \large Son Nguyen\\[1em]
    %\large \today
\end{center}
\subsection*{Problem A.9}
\begin{equation*}
    Aa = 
    \begin{pmatrix}
        -1 & 1 & i \\
        2 & 0 & 3 \\
        2i & -2i & 2
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        i \\
        2i \\
        2
    \end{pmatrix}
    = 
    \begin{pmatrix}
        3i \\
        6 + 2i \\
        6
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    a^\dagger b =
    \begin{pmatrix}
        -i & -2i & 2 
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
        2 \\
        1-i \\
        0
    \end{pmatrix}
    = 
    \begin{pmatrix}
        -2-4i
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    \tilde{a}Bb = 
    \begin{pmatrix}
        i & 2i & 2
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
        2 & 0 & -i \\
        0 & 1 & 0 \\
        i & 3 & 2 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 \\
        1-i \\
        0
    \end{pmatrix}
    =
    \begin{pmatrix}
        8+4i
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    a b^\dagger =
    \begin{pmatrix}
        i \\
        2i \\
        2
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 & 1+i & 0
    \end{pmatrix}
    = 
    \begin{pmatrix}
        2i & -1 + i & 0 \\
        4i & -2 + 2i & 0 \\
        4 & 2 + 2i & 0
    \end{pmatrix}
\end{equation*}
\subsection*{Problem A.15}
From equation (A.42) we have the rotation matrix can be epxpressed as: 
\[a' = Ta\] 
\begin{align*}
    \begin{pmatrix}
        i' \\
        j' \\
        k'
    \end{pmatrix}
    &= 
    T \cdot
    \begin{pmatrix}
        i \\
        j \\
        k
    \end{pmatrix} \\
\end{align*}
Since we are rotation about the x-axis, we have:
\begin{equation*}
    a'_x =
    \begin{cases}
    \hat{i'} = \hat{i} \\
    \hat{j'} = \cos(\theta) \hat{j} + \sin(\theta) \hat{k} \\
    \hat{k'} = -\sin(\theta) \hat{j} + \cos(\theta) \hat{k}
    \end{cases}
    \Rightarrow
    \begin{pmatrix}
        i' \\
        j' \\
        k'
    \end{pmatrix}
    =
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & \cos(\theta) & \sin(\theta) \\
        0 & -\sin(\theta) & \cos(\theta)
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        i \\
        j \\
        k
    \end{pmatrix}
\end{equation*}
\begin{equation}
    T_x =
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & \cos(\theta) & \sin(\theta) \\
        0 & -\sin(\theta) & \cos(\theta)
    \end{pmatrix}
\end{equation}
For the rotation about the y-axis, we have:
\begin{equation*}
    a'_y =
    \begin{cases}
        \hat{i'} = \cos(\theta) \hat{i} - \sin(\theta) \hat{k} \\
        \hat{j'} = \hat{j} \\
        \hat{k'} = \sin(\theta) \hat{i} + \cos(\theta) \hat{k}
    \end{cases}
    \Rightarrow
    \begin{pmatrix}
        i' \\
        j' \\
        k'
    \end{pmatrix}
    =
    \begin{pmatrix}
        \cos(\theta) & 0 & -\sin(\theta) \\
        0 & 1 & 0 \\
        \sin(\theta) & 0 & \cos(\theta)
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        i \\
        j \\
        k
    \end{pmatrix}
\end{equation*}
The rotation matrix about the y-axis is:
\begin{equation*}
    T_y = 
    \begin{pmatrix}
        \cos(\theta) & 0 & -\sin(\theta) \\
        0 & 1 & 0 \\
        \sin(\theta) & 0 & \cos(\theta)
    \end{pmatrix}
\end{equation*}
From equation (A.63), we have the formula for the change of basis matrix:
\[a^f = Sa^e\]
we have that: 
\begin{equation*}
    \begin{cases}
        \hat{i'} = \hat{j} \\
        \hat{j'} = -\hat{i} \\
        \hat{k'} = \hat{k}   
    \end{cases}
    \Rightarrow
    \begin{pmatrix}
        i' \\
        j' \\
        k'
    \end{pmatrix}
    =
    \begin{pmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        i \\
        j \\
        k
    \end{pmatrix}
\end{equation*}
Therefore, the matrix \(S\) is:
\begin{equation*}
    S = 
    \begin{pmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \Rightarrow
    S^{-1} =
    \begin{pmatrix}
        0 & -1 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
\end{equation*}
Now we calculate \(ST_x S^{-1}\) and \(ST_y S^{-1}\):
\begin{equation*}
    ST_x S^{-1} = 
    \begin{pmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & \cos(\theta) & \sin(\theta) \\
        0 & -\sin(\theta) & \cos(\theta)
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        0 & -1 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    =
    \begin{pmatrix}
        \cos(\theta) & 0 \sin(\theta) \\
        0 & 1 & 0 \\   
        -\sin(\theta) & 0 & \cos(\theta)
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    ST_y S^{-1} = 
    \begin{pmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \cos(\theta) & 0 & -\sin(\theta) \\
        0 & 1 & 0 \\
        \sin(\theta) & 0 & \cos(\theta)
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        0 & -1 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    =
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & \cos(\theta) & \sin(\theta) \\
        0 & -\sin(\theta) & \cos(\theta)
    \end{pmatrix}
\end{equation*}
\subsection*{Problem A.26}
From equation (A.82) matrix \(N\) is normal if:
\[[N^\dagger, N] = 0\]
For the matrix \(A\), we have: 
\begin{equation*}
    A^\dagger A - AA^\dagger =
    \begin{pmatrix}
        2 & 2 & -1 \\
        2 & -1 & 2 \\
        -1 & 2 & 2 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 & 2 & -1 \\
        2 & -1 & 2 \\
        -1 & 2 & 2 
    \end{pmatrix}
    - 
    \begin{pmatrix}
        2 & 2 & -1 \\
        2 & -1 & 2 \\
        -1 & 2 & 2 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 & 2 & -1 \\
        2 & -1 & 2 \\
        -1 & 2 & 2 
    \end{pmatrix}
    = 0
\end{equation*}
\[\Rightarrow A \text{ is diagonalizable}\]
For the matrix \(B\), we have:
\begin{equation*}
    B^\dagger B - BB^\dagger =
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2
    \end{pmatrix}
    -
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2
    \end{pmatrix}
    = 0 
\end{equation*}
\[\Rightarrow B \text{ is diagonalizable}\]
To test if they are commutable, we have:
\begin{equation*}
    AB = 
    \begin{pmatrix}
        2 & 2 & -1 \\
        2 & -1 & 2 \\
        -1 & 2 & 2 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2
    \end{pmatrix}
    = 
    \begin{pmatrix}
        0 & 9 & 0 \\
        9 & -9 & 9 \\
        0 & 9 & 0
    \end{pmatrix}
    =
    BA
\end{equation*}
\[\Rightarrow A \text{ and } B \text{ commute}\]
To find the eigenvalues and eigenvectors of \(A\), we have:
\begin{equation*}
    \text{det}(A - \lambda I) = 0 =
    \begin{vmatrix}
        2 - \lambda & 2 & -1 \\
        2 & -1 - \lambda & 2 \\
        -1 & 2 & 2 - \lambda
    \end{vmatrix}
\end{equation*}
\[\Rightarrow \lambda = \{-3, 3\}\]
To find the eigenvector corresponding to \(\lambda_1 = -3\), we have:
    \[(A - \lambda_1 I) v = 0\]
    \[\Rightarrow v_1 = 
    \begin{pmatrix}
        1 \\
        -2 \\
        1
    \end{pmatrix}
    \]
For the eigenvector corresponding to \(\lambda_2 = 3\), we have:
    \[(A - \lambda_2 I) v = 0\]
    \[\Rightarrow v_2 =
    \begin{pmatrix}
        2 \\
        1 \\
        0
    \end{pmatrix}
    \]

    \[\Rightarrow v_3 =
    \begin{pmatrix}
        -1 \\
        0 \\
        1
    \end{pmatrix}
    \]
Since \(\lambda_2\) coressponds to 2 eigenvectors, it is degenerate.
\\ \\
We can use the same formula to find the eigenvalues and eigenvectors of \(B\):
\[\text{det}(B - \lambda I) = 0 =
\begin{vmatrix}
    2 - \lambda & -1 & 2 \\
    -1 & 5 - \lambda & -1 \\
    2 & -1 & 2- \lambda 
\end{vmatrix}
\]
\[\Rightarrow \lambda = \{0, 3, 6\}\]
Now we check if the eigenvectors of \(A\) are also the eigenvector of \(B\):
\begin{equation*}
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        1 \\
        -2 \\
        1
    \end{pmatrix}
    = 
    \begin{pmatrix}
        6 \\
        -12 \\
        6
    \end{pmatrix}
    = 6v_1
\end{equation*}
\begin{equation*}
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        2 \\
        1 \\
        0
    \end{pmatrix}
    =
    \begin{pmatrix}
        3 \\
        3 \\
        3
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    \begin{pmatrix}
        2 & -1 & 2 \\
        -1 & 5 & -1 \\
        2 & -1 & 2 
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        -1 \\
        0 \\
        1
    \end{pmatrix}
    = 
    \begin{pmatrix}
        0 \\
        0 \\
        0
    \end{pmatrix}
    = 0v_3
\end{equation*}
So \(v_2\) is not a eigenvector of \(B\). For each eigenvalue of \(B\), we have:
\\ \\
For \(\lambda = 0\)
\[\Rightarrow v_3 = 
\begin{pmatrix}
    -1 \\
    0 \\
    1
\end{pmatrix}
\]
For \(\lambda = 3\)
\[\Rightarrow v_4 =
\begin{pmatrix}
    1 \\
    1 \\
    1
\end{pmatrix}
\]
For \(\lambda = 6\)
\[\Rightarrow v_1 =
\begin{pmatrix}
    1 \\
    -2 \\
    1
\end{pmatrix}
\]
We can test \(v_5\) on \(A\) to see if it is an eigenvector of \(A\):
\begin{equation*}
    \begin{pmatrix}
        2 & 2 & -1 \\
        2 & -1 & 2 \\
        -1 & 2 & 2
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        1 \\
        1 \\
        1
    \end{pmatrix}
    =
    \begin{pmatrix}
        3 \\
        3 \\
        3
    \end{pmatrix}
    = 3v_5
\end{equation*}
Therefore, \(v_4\) is an eigenvector of \(A\). So the eigenvectors \(v1, v3, v4\) of \(B\) are also eigenvectors of \(A\).
\subsection*{Problem A.29}
From equation (A.93) we have: 
\begin{align*}
    \text{det}(T) &= \lambda_1 \lambda_2 \dots \lambda_n \\
    \text{Tr}(T) &= \lambda_1 + \lambda_2 + \dots + \lambda_n
\end{align*}
\begin{equation}
    \text{det}
    \begin{vmatrix}
        2  & i & 1 \\
        -i & 2 & i \\
        1  & -i & 2
    \end{vmatrix} 
    = 0
\end{equation}
\begin{equation}
    \text{Tr}
    \begin{vmatrix}
        2  & i & 1 \\
        -i & 2 & i \\
        1  & -i & 2
    \end{vmatrix}
    = 2 + 2 + 2 = 6
\end{equation}
Find the eigenvalues of the matrix, we have that: 
for \(\lambda_1 = 0\), we have 
\(v_1 = A_1
\begin{pmatrix}
    -1 \\
    -i \\
    1
\end{pmatrix}
\).
For \(\lambda_2 = 3\), we have 
\(v_2 = A_2
\begin{pmatrix}
    i \\
    1 \\
    0
\end{pmatrix}
\), 
\(v_3 =
\begin{pmatrix}
    1 \\
    0 \\
    1
\end{pmatrix}
\)
They are consistent with the value with have in part a. 
\[\text{det}(T) = 3 \cdot 3 \cdot 0 = 0\]
\[\text{Tr}(T) = 3 + 3 + 0 = 6\]
To check orthogonality, we have:
\begin{equation*}
    v_1^\dagger v_2 = 
    \begin{pmatrix}
        -1 & i & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        i \\
        1 \\
        0
    \end{pmatrix}
    = 0
\end{equation*}
\begin{equation*}
    v_1^\dagger v_3 =
    \begin{pmatrix}
        -1 & i & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        1 \\
        0 \\
        1
    \end{pmatrix}
    =0
\end{equation*}
\begin{equation*}
    v_2^\dagger v_3 =
    \begin{pmatrix}
        -i & 1 & 0
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        1 \\
        0 \\
        1
    \end{pmatrix}
    = -i \neq 0
\end{equation*}
It means that vectors \(v_1\) is orthogonal to \(v_2\) and \(v_3\), \(v_2\) is not orthogonal to \(v_3\). Now we normalizing the three vectors:
\begin{align*}
    A_1^2|-1|^2 + |-i|^2 +|1|^2 &= 1 \\
    \Rightarrow A_1 &= \pm \frac{1}{\sqrt{3}} \\
    A_2^2|i|^2 + |1|^2 + |0|^2 &= 1 \\
    \Rightarrow A_2 &= \pm \frac{1}{\sqrt{2}} \\
    A_3^2|1|^2 + |0|^2 + |1|^2 &= 1 \\
    \Rightarrow A_3 &= \pm \frac{1}{\sqrt{2}}
\end{align*}
The three normalized vectors are:
\begin{equation*}
    v_1 = \frac{1}{\sqrt{3}}
    \begin{pmatrix}
        -1 \\
        -i \\
        1
    \end{pmatrix},
    v_2 = \frac{1}{\sqrt{2}}
    \begin{pmatrix}
        i \\
        1 \\
        0
    \end{pmatrix},
    v_3 = \frac{1}{\sqrt{2}}
    \begin{pmatrix}
        1 \\
        0 \\
        1
    \end{pmatrix}
\end{equation*}
The diagonalized version of \(T\) is:
\begin{equation*}
    \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 3 & 0 \\
        0 & 0 & 3
    \end{pmatrix}
\end{equation*}
proof:
\begin{equation*}
    \begin{pmatrix}
        2  & i & 1 \\
        -i & 2 & i \\
        1  & -i & 2
    \end{pmatrix}
    = 
    \begin{pmatrix}
        -1 & i & 1 \\
        -i & 1 & 0 \\
        1 & 0 & 1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 3 & 0 \\
        0 & 0 & 3
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \frac{-1}{3} & \frac{i}{3} & \frac{1}{3} \\
        \frac{-i}{3} & \frac{2}{3} & \frac{i}{3} \\
        \frac{1}{3} & \frac{-i}{3} & \frac{2}{3} 
    \end{pmatrix}
\end{equation*}
\subsection*{Problem A.30}
\begin{enumerate}[label=(\alph*)]
    \item we have:
    \begin{align*}
        \langle U \alpha | U \beta \rangle &= (U \alpha)^\dagger (U \beta) \\
        &= (a^\dagger U^\dagger) (U \beta) \\
        &= a^\dagger U^\dagger U \beta \\
        &= a^\dagger \beta \\
        &= \langle \alpha | \beta \rangle
    \end{align*}
    \item let \(\lambda\) be the eigenvalue, and \(|\alpha\rangle\) be the eigenvector of \(\hat{U}\), we have:
    \begin{align*}
        \hat{U} |\alpha\rangle &= \lambda |\alpha\rangle \\
    \end{align*}
    Taking the inner product of \(|\alpha\rangle\) with itself, we have:
    \begin{align*}
        \langle \alpha | \alpha \rangle &= \langle \alpha | \hat{U}^\dagger \hat{U} | \alpha \rangle \\
        &= \left(\langle \alpha | U^\dagger\right) \left(U | \alpha \rangle\right) \\
        &= \left(U |\alpha\rangle\right)^\dagger \left(U |\alpha\rangle\right) \\
        &= \left(\lambda |\alpha \rangle\right)^\dagger \left(\lambda |\alpha \rangle\right) \\
        &= \left(\lambda^* \langle \alpha |\right) (\lambda | \alpha \rangle) \\
        &= \lambda^* \lambda \langle \alpha | \alpha \rangle \\
        1 &= \lambda^* \lambda = |\lambda|^2 \\
        \Rightarrow |\lambda| &= 1
    \end{align*}
    \item given \(\lambda\) and \(\mu\) are the eigenvalues of \(\hat{U}\) with their respective eigenvectors \(|\alpha\rangle\) and \(|\beta\rangle\), we have:
    \begin{align*}
        \langle \alpha | \beta \rangle &= \langle \alpha | \hat{U}^\dagger \hat{U} | \beta \rangle \\
        &= \left(\langle \alpha | \hat{U}^\dagger\right) \left( \hat{U} | \beta \rangle\right) \\
        &= \left(\hat{U} |\alpha\rangle\right)^\dagger \left(\hat{U} |\beta\rangle\right) \\
        &= \left(\lambda |\alpha\rangle\right)^\dagger \left(\mu |\beta\rangle\right) \\
        &= \lambda^* \mu \langle \alpha | \beta \rangle \\
        0 &= \lambda^* \mu \langle \alpha | \beta \rangle - \langle \alpha | \beta \rangle \\
        &= (\lambda^* \mu - 1) \langle \alpha | \beta \rangle \\
    \end{align*}
    Now we need to show that \(\lambda^* \mu - 1  \neq 0\), multiply both sides by \(\lambda\):
    \begin{align*}
        \lambda^* \lambda \mu - \lambda &= 0 \\
        1 \mu - \lambda &= 0 \\
        \mu &= \lambda
    \end{align*}
    This is contradiction to the assumption that \(\mu \neq \lambda\). Therefore, \(\lambda^* \mu - 1 \neq 0\). It means that \(\langle \alpha | \beta \rangle = 0\), which implies that the eigenvectors of \(\hat{U}\) are orthogonal.
\end{enumerate}
\end{document}